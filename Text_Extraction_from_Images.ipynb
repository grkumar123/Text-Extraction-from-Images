{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Drive**"
      ],
      "metadata": {
        "id": "Bwfcgau5XStu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTdZiiBu1pR4",
        "outputId": "323d25d2-8702-47c3-d8f5-5400d7fa67b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "93wSQW-gXWjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import json\n",
        "from time import time\n",
        "import pickle\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
        "#from tensorflow.keras.layers.merge import add\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn import svm\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "from numpy import mean,std\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, NuSVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "from itertools import chain, product\n",
        "from itertools import zip_longest\n",
        "import string\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import imageio as iio\n",
        "import glob\n",
        "from skimage import io\n",
        "from IPython.display import Image\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dugTC1TB17Hc",
        "outputId": "1425a1eb-ceaa-40da-f210-dad5581d712a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Only for 1 data**"
      ],
      "metadata": {
        "id": "0slRpIK0hiKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import json\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "f1 = open('/content/word.txt', 'w+', encoding = 'utf-8')\n",
        "file_path = \"/content/drive/MyDrive/CVIT/Data/Gnhk/eng_AF_011.jpg\"\n",
        "\n",
        "for filename in os.listdir(file_path):\n",
        "  if filename.endswith(\".json\"):\n",
        "    print(filename)\n",
        "    \n",
        "    textdata = open(file_path+filename)\n",
        "    textdata = json.load(textdata)\n",
        "    print(textdata)\n",
        "    \n",
        "    imagename = filename.replace(\".json\", \".jpg\")\n",
        "    #print(imagename)\n",
        "\n",
        "    #img = Image.open('/content/drive/MyDrive/CVIT/Data/gnhk/eng_AF_011.jpg')\n",
        "\n",
        "\n",
        "    #img = cv2.imread(file_path+imagename) \n",
        "\n",
        "    print('\\n')\n",
        "    \n",
        "    length = len(textdata)\n",
        "    print(length)\n",
        "\n",
        "    \n",
        "    #for k,v in all_dimensions[j].items():\n",
        "\n",
        "    for i in range(len(textdata)):\n",
        "      \n",
        "      all_textdata = [item['text'] for item in textdata]\n",
        "      all_textdata\n",
        "      all_dimensions = [it['polygon'] for it in textdata]\n",
        "      #for j in range(len(all_dimensions)):\n",
        "      gyan = all_dimensions[i]\n",
        "      for xx in gyan.items():\n",
        "        x_values = []  #Store min and max x values\n",
        "        y_values = []  #Store min and max y values\n",
        "\n",
        "        x_values.append(gyan.get('x0'))\n",
        "        x_values.append(gyan.get('x1'))\n",
        "        x_values.append(gyan.get('x2'))\n",
        "        x_values.append(gyan.get('x3'))\n",
        "\n",
        "        y_values.append(gyan.get('y0'))\n",
        "        y_values.append(gyan.get('y1'))\n",
        "        y_values.append(gyan.get('y2'))\n",
        "        y_values.append(gyan.get('y3'))\n",
        "\n",
        "        min_x = min(x_values)\n",
        "        min_y = min(y_values)\n",
        "        max_x = max(x_values)\n",
        "        max_y = max(y_values)\n",
        "          \n",
        "        #print(min_x), print(min_y)\n",
        "        #print(max_x), print(max_y)\n",
        "\n",
        "        img = Image.open(file_path + imagename)\n",
        "        img = img.convert(\"RGB\")\n",
        "        cropped = img.crop((min_x,min_y,max_x,max_y))\n",
        "\n",
        "        #cropped.save(path+imagename+str(i)+\".jpg\", img)\n",
        "      crop_image_name = imagename.replace(\".jpg\",\"\")\n",
        "      crop_image_name1 = crop_image_name + \"_\"+ str(i) + \".jpg\"\n",
        "      cropped.save('/content/'+crop_image_name1)\n",
        "      f1.write(str(crop_image_name1))\n",
        "      f1.write(\" \")\n",
        "      f1.write(str(all_textdata[i]))\n",
        "      f1.write(\"\\n\")\n",
        "\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "JIO2hJ9e1wdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For all data**"
      ],
      "metadata": {
        "id": "KWUkO-Lxhlt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import json\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "f1 = open('/content/word.txt', 'w+', encoding = 'utf-8')\n",
        "file_path = \"/content/drive/MyDrive/CVIT/Data/gnhk/\"\n",
        "\n",
        "for filename in os.listdir(file_path):\n",
        "  if filename.endswith(\".json\"):\n",
        "    print(filename)\n",
        "    \n",
        "    textdata = open(file_path+filename)\n",
        "    textdata = json.load(textdata)\n",
        "    print(textdata)\n",
        "    \n",
        "    imagename = filename.replace(\".json\", \".jpg\")\n",
        "    #print(imagename)\n",
        "\n",
        "    #img = Image.open('/content/drive/MyDrive/CVIT/Data/gnhk/eng_AF_003.jpg')\n",
        "\n",
        "\n",
        "    #img = cv2.imread(file_path+imagename) \n",
        "\n",
        "    print('\\n')\n",
        "    \n",
        "    length = len(textdata)\n",
        "    print(length)\n",
        "\n",
        "    \n",
        "    #for k,v in all_dimensions[j].items():\n",
        "\n",
        "    for i in range(len(textdata)):\n",
        "      \n",
        "      all_textdata = [item['text'] for item in textdata]\n",
        "      all_textdata\n",
        "      all_dimensions = [it['polygon'] for it in textdata]\n",
        "      #for j in range(len(all_dimensions)):\n",
        "      gyan = all_dimensions[i]\n",
        "      for xx in gyan.items():\n",
        "        x_values = []  #Store min and max x values\n",
        "        y_values = []  #Store min and max y values\n",
        "\n",
        "        x_values.append(gyan.get('x0'))\n",
        "        x_values.append(gyan.get('x1'))\n",
        "        x_values.append(gyan.get('x2'))\n",
        "        x_values.append(gyan.get('x3'))\n",
        "\n",
        "        y_values.append(gyan.get('y0'))\n",
        "        y_values.append(gyan.get('y1'))\n",
        "        y_values.append(gyan.get('y2'))\n",
        "        y_values.append(gyan.get('y3'))\n",
        "\n",
        "        min_x = min(x_values)\n",
        "        min_y = min(y_values)\n",
        "        max_x = max(x_values)\n",
        "        max_y = max(y_values)\n",
        "          \n",
        "        #print(min_x), print(min_y)\n",
        "        #print(max_x), print(max_y)\n",
        "\n",
        "        img = Image.open(file_path + imagename)\n",
        "        cropped = img.crop((min_x,min_y,max_x,max_y))\n",
        "\n",
        "        #cropped.save(path+imagename+str(i)+\".jpg\", img)\n",
        "      crop_image_name = imagename.replace(\".jpg\",\"\")\n",
        "      crop_image_name1 = crop_image_name + \"_\"+ str(i) + \".jpg\"\n",
        "      cropped.save('/content/'+crop_image_name1)\n",
        "      f1.write(str(crop_image_name1))\n",
        "      f1.write(\" \")\n",
        "      f1.write(str(all_textdata[i]))\n",
        "      f1.write(\"\\n\")\n",
        "\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "PXbeQe9i1wiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**For Training Data**"
      ],
      "metadata": {
        "id": "jWcnP9twPxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import json\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "f1 = open('/content/drive/MyDrive/CVIT/Data/GNHK/training_word.txt', 'w+', encoding = 'utf-8')\n",
        "file_path = \"/content/drive/MyDrive/CVIT/Data/GNHK/train/\"\n",
        "\n",
        "for filename in os.listdir(file_path):\n",
        "  if filename.endswith(\".json\"):\n",
        "    print(filename)\n",
        "    \n",
        "    textdata = open(file_path+filename)\n",
        "    textdata = json.load(textdata)\n",
        "    print(textdata)\n",
        "    \n",
        "    imagename = filename.replace(\".json\", \".jpg\")\n",
        "    #print(imagename)\n",
        "\n",
        "    #img = Image.open('/content/drive/MyDrive/CVIT/Data/gnhk/eng_AF_003.jpg')\n",
        "\n",
        "\n",
        "    #img = cv2.imread(file_path+imagename) \n",
        "\n",
        "    print('\\n')\n",
        "    \n",
        "    length = len(textdata)\n",
        "    print(length)\n",
        "\n",
        "    \n",
        "    #for k,v in all_dimensions[j].items():\n",
        "\n",
        "    for i in range(len(textdata)):\n",
        "      \n",
        "      all_textdata = [item['text'] for item in textdata]\n",
        "      all_textdata\n",
        "      all_dimensions = [it['polygon'] for it in textdata]\n",
        "      #for j in range(len(all_dimensions)):\n",
        "      gyan = all_dimensions[i]\n",
        "      for xx in gyan.items():\n",
        "        x_values = []  #Store min and max x values\n",
        "        y_values = []  #Store min and max y values\n",
        "\n",
        "        x_values.append(gyan.get('x0'))\n",
        "        x_values.append(gyan.get('x1'))\n",
        "        x_values.append(gyan.get('x2'))\n",
        "        x_values.append(gyan.get('x3'))\n",
        "\n",
        "        y_values.append(gyan.get('y0'))\n",
        "        y_values.append(gyan.get('y1'))\n",
        "        y_values.append(gyan.get('y2'))\n",
        "        y_values.append(gyan.get('y3'))\n",
        "\n",
        "        min_x = min(x_values)\n",
        "        min_y = min(y_values)\n",
        "        max_x = max(x_values)\n",
        "        max_y = max(y_values)\n",
        "          \n",
        "        #print(min_x), print(min_y)\n",
        "        #print(max_x), print(max_y)\n",
        "\n",
        "        img = Image.open(file_path + imagename)\n",
        "        img = img.convert(\"RGB\")\n",
        "        cropped = img.crop((min_x,min_y,max_x,max_y))\n",
        "\n",
        "        #cropped.save(path+imagename+str(i)+\".jpg\", img)\n",
        "      crop_image_name = imagename.replace(\".jpg\",\"\")\n",
        "      crop_image_name1 = crop_image_name + \"_\"+ str(i) + \".jpg\"\n",
        "      cropped.save('/content/drive/MyDrive/CVIT/Data/GNHK/Output1/'+crop_image_name1)\n",
        "      f1.write(str(crop_image_name1))\n",
        "      f1.write(\" \")\n",
        "      f1.write(str(all_textdata[i]))\n",
        "      f1.write(\"\\n\")\n",
        "\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "W-TKO9ViPQkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**For Testing Data**"
      ],
      "metadata": {
        "id": "Ep501_vNmM1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import json\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "f1 = open('/content/drive/MyDrive/CVIT/Data/GNHK/testing_word.txt', 'w+', encoding = 'utf-8')\n",
        "file_path = \"/content/drive/MyDrive/CVIT/Data/GNHK/test/\"\n",
        "\n",
        "for filename in os.listdir(file_path):\n",
        "  if filename.endswith(\".json\"):\n",
        "    print(filename)\n",
        "    \n",
        "    textdata = open(file_path+filename)\n",
        "    textdata = json.load(textdata)\n",
        "    print(textdata)\n",
        "    \n",
        "    imagename = filename.replace(\".json\", \".jpg\")\n",
        "    #print(imagename)\n",
        "\n",
        "    #img = Image.open('/content/drive/MyDrive/CVIT/Data/gnhk/eng_AF_003.jpg')\n",
        "\n",
        "\n",
        "    #img = cv2.imread(file_path+imagename) \n",
        "\n",
        "    print('\\n')\n",
        "    \n",
        "    length = len(textdata)\n",
        "    print(length)\n",
        "\n",
        "    \n",
        "    #for k,v in all_dimensions[j].items():\n",
        "\n",
        "    for i in range(len(textdata)):\n",
        "      \n",
        "      all_textdata = [item['text'] for item in textdata]\n",
        "      all_textdata\n",
        "      all_dimensions = [it['polygon'] for it in textdata]\n",
        "      #for j in range(len(all_dimensions)):\n",
        "      gyan = all_dimensions[i]\n",
        "      for xx in gyan.items():\n",
        "        x_values = []  #Store min and max x values\n",
        "        y_values = []  #Store min and max y values\n",
        "\n",
        "        x_values.append(gyan.get('x0'))\n",
        "        x_values.append(gyan.get('x1'))\n",
        "        x_values.append(gyan.get('x2'))\n",
        "        x_values.append(gyan.get('x3'))\n",
        "\n",
        "        y_values.append(gyan.get('y0'))\n",
        "        y_values.append(gyan.get('y1'))\n",
        "        y_values.append(gyan.get('y2'))\n",
        "        y_values.append(gyan.get('y3'))\n",
        "\n",
        "        min_x = min(x_values)\n",
        "        min_y = min(y_values)\n",
        "        max_x = max(x_values)\n",
        "        max_y = max(y_values)\n",
        "          \n",
        "        #print(min_x), print(min_y)\n",
        "        #print(max_x), print(max_y)\n",
        "\n",
        "        img = Image.open(file_path + imagename)\n",
        "        img = img.convert(\"RGB\")\n",
        "        cropped = img.crop((min_x,min_y,max_x,max_y))\n",
        "\n",
        "        #cropped.save(path+imagename+str(i)+\".jpg\", img)\n",
        "      crop_image_name = imagename.replace(\".jpg\",\"\")\n",
        "      crop_image_name1 = crop_image_name + \"_\"+ str(i) + \".jpg\"\n",
        "      cropped.save('/content/drive/MyDrive/CVIT/Data/GNHK/Output2/'+crop_image_name1)\n",
        "      f1.write(str(crop_image_name1))\n",
        "      f1.write(\" \")\n",
        "      f1.write(str(all_textdata[i]))\n",
        "      f1.write(\"\\n\")\n",
        "\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "fM6udhJ8PQmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}